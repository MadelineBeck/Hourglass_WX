{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b672ff-9027-4ef9-9e59-1edbe787e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "import datetime as dt\n",
    "np.seterr(divide='ignore', invalid='ignore')  # i do not really like that, but division by zero is going to nan silently\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50dc4b1-a32b-49f9-a5a6-f6c0e154da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory to the Hourglass Weather Station data\n",
    "os.chdir('/Users/f67f911/Desktop/Hourglass_WX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58c688d-5b75-491c-b2f8-84e8dcb3413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Earth's gravitational acceleration constant\n",
    "EARTH_GRAVITATIONAL_ACCELERATION = 9.81  # m/s^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00409c0d-df79-4b7d-9bc9-014a265821e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is ported from R code which was written by Winkler et al. 2020 'Snow Water Equivalents exclusively from Snow Heights and their temporal Changes: The âˆ†SNOW.MODEL'\n",
      "Using parameters:\n",
      "rho.max  = 434.4691095\n",
      "rho.null = 88.63272262\n",
      "c.ov     = 0.0005104722\n",
      "k.ov     = 0.37856737\n",
      "k        = 0.02993175\n",
      "tau      = 0.02362476\n",
      "eta.null = 8523356\n",
      "day 1: \n",
      "day 2: \n",
      "day 3: \n",
      "day 4: \n",
      "day 5: \n",
      "day 6: \n",
      "day 7: \n",
      "day 8: \n",
      "day 9: \n",
      "day 10: \n",
      "day 11: \n",
      "day 12: produce layer 1 \n",
      "day 13: create new layer 1 \n",
      "day 14: create new layer 2 \n",
      "day 15: scaling: \n",
      "day 16: drenching: melt compaction \n",
      "day 17: drenching: melt compaction \n",
      "day 18: scaling: \n",
      "day 19: scaling: \n",
      "day 20: scaling: \n",
      "day 21: scaling: \n",
      "day 22: drenching: melt compaction \n",
      "day 23: scaling: \n",
      "day 24: scaling: \n",
      "day 25: scaling: \n",
      "day 26: create new layer 3 \n",
      "day 27: scaling: \n",
      "day 28: scaling: \n",
      "day 29: scaling: \n",
      "day 30: scaling: \n",
      "day 31: scaling: \n",
      "day 32: scaling: \n",
      "day 33: scaling: \n",
      "day 34: scaling: \n",
      "day 35: scaling: \n",
      "day 36: scaling: \n",
      "day 37: create new layer 4 \n",
      "day 38: create new layer 5 \n",
      "day 39: drenching: melt compaction \n",
      "day 40: scaling: \n",
      "day 41: create new layer 6 \n",
      "day 42: scaling: \n",
      "day 43: scaling: \n",
      "day 44: drenching: melt compaction \n",
      "day 45: scaling: \n",
      "day 46: scaling: \n",
      "day 47: scaling: \n",
      "day 48: scaling: \n",
      "day 49: scaling: \n",
      "day 50: scaling: \n",
      "day 51: create new layer 7 \n",
      "day 52: scaling: \n",
      "day 53: scaling: \n",
      "day 54: scaling: \n",
      "day 55: create new layer 8 \n",
      "day 56: scaling: \n",
      "day 57: scaling: \n",
      "day 58: scaling: \n",
      "day 59: scaling: \n",
      "day 60: scaling: \n",
      "day 61: create new layer 9 \n",
      "day 62: scaling: \n",
      "day 63: drenching: melt compaction \n",
      "day 64: scaling: \n",
      "day 65: scaling: \n",
      "day 66: scaling: \n",
      "day 67: create new layer 10 \n",
      "day 68: create new layer 11 \n",
      "day 69: scaling: \n",
      "day 70: scaling: \n",
      "day 71: create new layer 12 \n",
      "day 72: scaling: \n",
      "day 73: scaling: \n",
      "day 74: scaling: \n",
      "day 75: create new layer 13 \n",
      "day 76: create new layer 14 \n",
      "day 77: create new layer 15 \n",
      "day 78: create new layer 16 \n",
      "day 79: scaling: \n",
      "day 80: scaling: \n",
      "day 81: create new layer 17 \n",
      "day 82: scaling: \n",
      "day 83: scaling: \n",
      "day 84: scaling: \n",
      "day 85: create new layer 18 \n",
      "day 86: scaling: \n",
      "day 87: create new layer 19 \n",
      "day 88: create new layer 20 \n",
      "day 89: scaling: \n",
      "day 90: create new layer 21 \n",
      "day 91: scaling: \n",
      "day 92: create new layer 22 \n",
      "day 93: scaling: \n",
      "day 94: scaling: \n",
      "day 95: drenching: melt compaction \n",
      "day 96: drenching: melt compaction \n",
      "day 97: create new layer 23 \n",
      "day 98: create new layer 24 \n",
      "day 99: create new layer 25 \n",
      "day 100: create new layer 26 \n",
      "day 101: scaling: \n",
      "day 102: scaling: \n",
      "day 103: scaling: \n",
      "day 104: scaling: \n",
      "day 105: scaling: \n",
      "day 106: scaling: \n",
      "day 107: scaling: \n",
      "day 108: scaling: \n",
      "day 109: create new layer 27 \n",
      "day 110: create new layer 28 \n",
      "day 111: create new layer 29 \n",
      "day 112: create new layer 30 \n",
      "day 113: create new layer 31 \n",
      "day 114: create new layer 32 \n",
      "day 115: scaling: \n",
      "day 116: scaling: \n",
      "day 117: create new layer 33 \n",
      "day 118: scaling: \n",
      "day 119: scaling: \n",
      "day 120: scaling: \n",
      "day 121: scaling: \n",
      "day 122: scaling: \n",
      "day 123: scaling: \n",
      "day 124: create new layer 34 \n",
      "day 125: create new layer 35 \n",
      "day 126: scaling: \n",
      "day 127: scaling: \n",
      "day 128: scaling: \n",
      "day 129: scaling: \n",
      "day 130: scaling: \n",
      "day 131: scaling: \n",
      "day 132: scaling: \n",
      "day 133: scaling: \n",
      "day 134: scaling: \n",
      "day 135: scaling: \n",
      "day 136: scaling: \n",
      "day 137: scaling: \n",
      "day 138: create new layer 36 \n",
      "day 139: create new layer 37 \n",
      "day 140: create new layer 38 \n",
      "day 141: scaling: \n",
      "day 142: scaling: \n",
      "day 143: create new layer 39 \n",
      "day 144: create new layer 40 \n",
      "day 145: scaling: \n",
      "day 146: scaling: \n",
      "day 147: scaling: \n",
      "day 148: scaling: \n",
      "day 149: scaling: \n",
      "day 150: scaling: \n",
      "day 151: scaling: \n",
      "day 152: scaling: \n",
      "day 153: scaling: \n",
      "day 154: scaling: \n",
      "day 155: scaling: \n",
      "day 156: create new layer 41 \n",
      "day 157: create new layer 42 \n",
      "day 158: scaling: \n",
      "day 159: create new layer 43 \n",
      "day 160: scaling: \n",
      "day 161: scaling: \n",
      "day 162: scaling: \n",
      "day 163: scaling: \n",
      "day 164: create new layer 44 \n",
      "day 165: create new layer 45 \n",
      "day 166: create new layer 46 \n",
      "day 167: scaling: \n",
      "day 168: scaling: \n",
      "day 169: scaling: \n",
      "day 170: scaling: \n",
      "day 171: create new layer 47 \n",
      "day 172: create new layer 48 \n",
      "day 173: scaling: \n",
      "day 174: scaling: \n",
      "day 175: scaling: \n",
      "day 176: scaling: \n",
      "day 177: scaling: \n",
      "day 178: scaling: \n",
      "day 179: scaling: \n",
      "day 180: scaling: \n",
      "day 181: scaling: \n",
      "day 182: scaling: \n",
      "day 183: scaling: \n",
      "day 184: create new layer 49 \n",
      "day 185: create new layer 50 \n",
      "day 186: scaling: \n",
      "day 187: scaling: \n",
      "day 188: scaling: \n",
      "day 189: scaling: \n",
      "day 190: scaling: \n",
      "day 191: scaling: \n",
      "day 192: scaling: \n",
      "day 193: create new layer 51 \n",
      "day 194: create new layer 52 \n",
      "day 195: create new layer 53 \n",
      "day 196: scaling: \n",
      "day 197: create new layer 54 \n",
      "day 198: scaling: \n",
      "day 199: create new layer 55 \n",
      "day 200: scaling: \n",
      "day 201: scaling: \n",
      "day 202: scaling: \n",
      "day 203: scaling: \n",
      "day 204: create new layer 56 \n",
      "day 205: create new layer 57 \n",
      "day 206: scaling: \n",
      "day 207: scaling: \n",
      "day 208: drenching: melt compaction \n",
      "day 209: scaling: \n",
      "day 210: scaling: \n",
      "day 211: create new layer 58 \n",
      "day 212: scaling: \n",
      "day 213: scaling: \n",
      "day 214: scaling: \n",
      "day 215: scaling: \n",
      "day 216: scaling: \n",
      "day 217: drenching: melt compaction \n",
      "day 218: drenching: melt compaction \n",
      "day 219: scaling: \n",
      "day 220: create new layer 59 \n",
      "day 221: create new layer 60 \n",
      "day 222: drenching: melt compaction \n",
      "day 223: drenching: melt compaction \n",
      "day 224: scaling: \n",
      "day 225: scaling: \n",
      "day 226: scaling: \n",
      "day 227: scaling: \n",
      "day 228: drenching: melt compaction \n",
      "day 229: drenching: melt compaction \n",
      "day 230: drenching: melt no further compaction runoff \n",
      "day 231: scaling:  runoff\n",
      "day 232: create new layer 61 \n",
      "day 233: scaling: \n",
      "day 234: scaling: \n",
      "day 235: scaling: \n",
      "day 236: drenching: melt no further compaction runoff \n",
      "day 237: drenching: melt no further compaction runoff \n",
      "day 238: drenching: melt no further compaction runoff \n",
      "day 239: drenching: melt no further compaction runoff \n",
      "day 240: drenching: melt no further compaction runoff \n",
      "day 241: drenching: melt no further compaction runoff \n",
      "day 242: scaling:  runoff\n",
      "day 243: scaling:  runoff\n",
      "day 244: drenching: melt no further compaction runoff \n",
      "day 245: drenching: melt no further compaction runoff \n",
      "day 246: drenching: melt no further compaction runoff \n",
      "day 247: drenching: melt no further compaction runoff \n",
      "day 248: drenching: melt no further compaction runoff \n",
      "day 249: drenching: melt no further compaction runoff \n",
      "day 250: drenching: melt no further compaction runoff \n",
      "day 251: drenching: melt no further compaction runoff \n",
      "day 252: drenching: melt no further compaction runoff \n",
      "day 253: drenching: melt no further compaction runoff \n",
      "day 254: drenching: melt no further compaction runoff \n",
      "day 255: drenching: melt no further compaction runoff \n",
      "day 256: drenching: melt no further compaction runoff \n",
      "day 257: scaling:  runoff\n",
      "day 258: drenching: melt no further compaction runoff \n",
      "day 259: drenching: melt no further compaction runoff \n",
      "day 260: scaling:  runoff\n",
      "day 261: scaling:  runoff\n",
      "day 262: scaling:  runoff\n",
      "day 263: scaling: \n",
      "day 264: scaling: \n",
      "day 265: scaling: \n",
      "day 266: scaling: \n",
      "day 267: scaling: \n",
      "day 268: runoff\n",
      "day 269: \n",
      "day 270: \n",
      "day 271: \n",
      "day 272: \n",
      "day 273: \n",
      "Using parameters:\n",
      "rho.max  = 434.4691095\n",
      "rho.null = 88.63272262\n",
      "c.ov     = 0.0005104722\n",
      "k.ov     = 0.37856737\n",
      "k        = 0.02993175\n",
      "tau      = 0.02362476\n",
      "eta.null = 8523356\n",
      "day 1: \n",
      "day 2: \n",
      "day 3: \n",
      "day 4: \n",
      "day 5: \n",
      "day 6: \n",
      "day 7: \n",
      "day 8: \n",
      "day 9: \n",
      "day 10: \n",
      "day 11: \n",
      "day 12: produce layer 1 \n",
      "day 13: create new layer 1 \n",
      "day 14: create new layer 2 \n",
      "day 15: scaling: \n",
      "day 16: drenching: melt compaction \n",
      "day 17: drenching: melt compaction \n",
      "day 18: scaling: \n",
      "day 19: scaling: \n",
      "day 20: scaling: \n",
      "day 21: scaling: \n",
      "day 22: drenching: melt compaction \n",
      "day 23: scaling: \n",
      "day 24: scaling: \n",
      "day 25: scaling: \n",
      "day 26: create new layer 3 \n",
      "day 27: scaling: \n",
      "day 28: scaling: \n",
      "day 29: scaling: \n",
      "day 30: scaling: \n",
      "day 31: scaling: \n",
      "day 32: scaling: \n",
      "day 33: scaling: \n",
      "day 34: scaling: \n",
      "day 35: scaling: \n",
      "day 36: scaling: \n",
      "day 37: create new layer 4 \n",
      "day 38: create new layer 5 \n",
      "day 39: drenching: melt compaction \n",
      "day 40: scaling: \n",
      "day 41: create new layer 6 \n",
      "day 42: scaling: \n",
      "day 43: scaling: \n",
      "day 44: drenching: melt compaction \n",
      "day 45: scaling: \n",
      "day 46: scaling: \n",
      "day 47: scaling: \n",
      "day 48: scaling: \n",
      "day 49: scaling: \n",
      "day 50: scaling: \n",
      "day 51: create new layer 7 \n",
      "day 52: scaling: \n",
      "day 53: scaling: \n",
      "day 54: scaling: \n",
      "day 55: create new layer 8 \n",
      "day 56: scaling: \n",
      "day 57: scaling: \n",
      "day 58: scaling: \n",
      "day 59: scaling: \n",
      "day 60: scaling: \n",
      "day 61: create new layer 9 \n",
      "day 62: scaling: \n",
      "day 63: drenching: melt compaction \n",
      "day 64: scaling: \n",
      "day 65: scaling: \n",
      "day 66: scaling: \n",
      "day 67: create new layer 10 \n",
      "day 68: create new layer 11 \n",
      "day 69: scaling: \n",
      "day 70: scaling: \n",
      "day 71: create new layer 12 \n",
      "day 72: scaling: \n",
      "day 73: scaling: \n",
      "day 74: scaling: \n",
      "day 75: create new layer 13 \n",
      "day 76: create new layer 14 \n",
      "day 77: create new layer 15 \n",
      "day 78: create new layer 16 \n",
      "day 79: scaling: \n",
      "day 80: scaling: \n",
      "day 81: create new layer 17 \n",
      "day 82: scaling: \n",
      "day 83: scaling: \n",
      "day 84: scaling: \n",
      "day 85: create new layer 18 \n",
      "day 86: scaling: \n",
      "day 87: create new layer 19 \n",
      "day 88: create new layer 20 \n",
      "day 89: scaling: \n",
      "day 90: create new layer 21 \n",
      "day 91: scaling: \n",
      "day 92: create new layer 22 \n",
      "day 93: scaling: \n",
      "day 94: scaling: \n",
      "day 95: drenching: melt compaction \n",
      "day 96: drenching: melt compaction \n",
      "day 97: create new layer 23 \n",
      "day 98: create new layer 24 \n",
      "day 99: create new layer 25 \n",
      "day 100: create new layer 26 \n",
      "day 101: scaling: \n",
      "day 102: scaling: \n",
      "day 103: scaling: \n",
      "day 104: scaling: \n",
      "day 105: scaling: \n",
      "day 106: scaling: \n",
      "day 107: scaling: \n",
      "day 108: scaling: \n",
      "day 109: create new layer 27 \n",
      "day 110: create new layer 28 \n",
      "day 111: create new layer 29 \n",
      "day 112: create new layer 30 \n",
      "day 113: create new layer 31 \n",
      "day 114: create new layer 32 \n",
      "day 115: scaling: \n",
      "day 116: scaling: \n",
      "day 117: create new layer 33 \n",
      "day 118: scaling: \n",
      "day 119: scaling: \n",
      "day 120: scaling: \n",
      "day 121: scaling: \n",
      "day 122: scaling: \n",
      "day 123: scaling: \n",
      "day 124: create new layer 34 \n",
      "day 125: create new layer 35 \n",
      "day 126: scaling: \n",
      "day 127: scaling: \n",
      "day 128: scaling: \n",
      "day 129: scaling: \n",
      "day 130: scaling: \n",
      "day 131: scaling: \n",
      "day 132: scaling: \n",
      "day 133: scaling: \n",
      "day 134: scaling: \n",
      "day 135: scaling: \n",
      "day 136: scaling: \n",
      "day 137: scaling: \n",
      "day 138: create new layer 36 \n",
      "day 139: create new layer 37 \n",
      "day 140: create new layer 38 \n",
      "day 141: scaling: \n",
      "day 142: scaling: \n",
      "day 143: create new layer 39 \n",
      "day 144: create new layer 40 \n",
      "day 145: scaling: \n",
      "day 146: scaling: \n",
      "day 147: scaling: \n",
      "day 148: scaling: \n",
      "day 149: scaling: \n",
      "day 150: scaling: \n",
      "day 151: scaling: \n",
      "day 152: scaling: \n",
      "day 153: scaling: \n",
      "day 154: scaling: \n",
      "day 155: scaling: \n",
      "day 156: create new layer 41 \n",
      "day 157: create new layer 42 \n",
      "day 158: scaling: \n",
      "day 159: create new layer 43 \n",
      "day 160: scaling: \n",
      "day 161: scaling: \n",
      "day 162: scaling: \n",
      "day 163: scaling: \n",
      "day 164: create new layer 44 \n",
      "day 165: create new layer 45 \n",
      "day 166: create new layer 46 \n",
      "day 167: scaling: \n",
      "day 168: scaling: \n",
      "day 169: scaling: \n",
      "day 170: scaling: \n",
      "day 171: create new layer 47 \n",
      "day 172: create new layer 48 \n",
      "day 173: scaling: \n",
      "day 174: scaling: \n",
      "day 175: scaling: \n",
      "day 176: scaling: \n",
      "day 177: scaling: \n",
      "day 178: scaling: \n",
      "day 179: scaling: \n",
      "day 180: scaling: \n",
      "day 181: scaling: \n",
      "day 182: scaling: \n",
      "day 183: scaling: \n",
      "day 184: create new layer 49 \n",
      "day 185: create new layer 50 \n",
      "day 186: scaling: \n",
      "day 187: scaling: \n",
      "day 188: scaling: \n",
      "day 189: scaling: \n",
      "day 190: scaling: \n",
      "day 191: scaling: \n",
      "day 192: scaling: \n",
      "day 193: create new layer 51 \n",
      "day 194: create new layer 52 \n",
      "day 195: create new layer 53 \n",
      "day 196: scaling: \n",
      "day 197: create new layer 54 \n",
      "day 198: scaling: \n",
      "day 199: create new layer 55 \n",
      "day 200: scaling: \n",
      "day 201: scaling: \n",
      "day 202: scaling: \n",
      "day 203: scaling: \n",
      "day 204: create new layer 56 \n",
      "day 205: create new layer 57 \n",
      "day 206: scaling: \n",
      "day 207: scaling: \n",
      "day 208: drenching: melt compaction \n",
      "day 209: scaling: \n",
      "day 210: scaling: \n",
      "day 211: create new layer 58 \n",
      "day 212: scaling: \n",
      "day 213: scaling: \n",
      "day 214: scaling: \n",
      "day 215: scaling: \n",
      "day 216: scaling: \n",
      "day 217: drenching: melt compaction \n",
      "day 218: drenching: melt compaction \n",
      "day 219: scaling: \n",
      "day 220: create new layer 59 \n",
      "day 221: create new layer 60 \n",
      "day 222: drenching: melt compaction \n",
      "day 223: drenching: melt compaction \n",
      "day 224: scaling: \n",
      "day 225: scaling: \n",
      "day 226: scaling: \n",
      "day 227: scaling: \n",
      "day 228: drenching: melt compaction \n",
      "day 229: drenching: melt compaction \n",
      "day 230: drenching: melt no further compaction runoff \n",
      "day 231: scaling:  runoff\n",
      "day 232: create new layer 61 \n",
      "day 233: scaling: \n",
      "day 234: scaling: \n",
      "day 235: scaling: \n",
      "day 236: drenching: melt no further compaction runoff \n",
      "day 237: drenching: melt no further compaction runoff \n",
      "day 238: drenching: melt no further compaction runoff \n",
      "day 239: drenching: melt no further compaction runoff \n",
      "day 240: drenching: melt no further compaction runoff \n",
      "day 241: drenching: melt no further compaction runoff \n",
      "day 242: scaling:  runoff\n",
      "day 243: scaling:  runoff\n",
      "day 244: drenching: melt no further compaction runoff \n",
      "day 245: drenching: melt no further compaction runoff \n",
      "day 246: drenching: melt no further compaction runoff \n",
      "day 247: drenching: melt no further compaction runoff \n",
      "day 248: drenching: melt no further compaction runoff \n",
      "day 249: drenching: melt no further compaction runoff \n",
      "day 250: drenching: melt no further compaction runoff \n",
      "day 251: drenching: melt no further compaction runoff \n",
      "day 252: drenching: melt no further compaction runoff \n",
      "day 253: drenching: melt no further compaction runoff \n",
      "day 254: drenching: melt no further compaction runoff \n",
      "day 255: drenching: melt no further compaction runoff \n",
      "day 256: drenching: melt no further compaction runoff \n",
      "day 257: scaling:  runoff\n",
      "day 258: drenching: melt no further compaction runoff \n",
      "day 259: drenching: melt no further compaction runoff \n",
      "day 260: scaling:  runoff\n",
      "day 261: scaling:  runoff\n",
      "day 262: scaling:  runoff\n",
      "day 263: scaling: \n",
      "day 264: scaling: \n",
      "day 265: scaling: \n",
      "day 266: scaling: \n",
      "day 267: scaling: \n",
      "day 268: runoff\n",
      "day 269: \n",
      "day 270: \n",
      "day 271: \n",
      "day 272: \n",
      "day 273: \n",
      "Mean 312.23937791990716\n",
      "Max 616.1524841990026\n",
      "Sum 85241.35017213463\n"
     ]
    }
   ],
   "source": [
    "# Run the created Delta Snow approach in python using the SNOTEL Site Specific rho max and rho null values\n",
    "class SnowToSwe:\n",
    "    def __init__(self, rho_max=434.4691095, rho_null=88.63272262, c_ov=0.0005104722, k_ov=0.37856737, k=0.02993175,\n",
    "                 tau=0.02362476, eta_null=8523356):\n",
    "        \"\"\"\n",
    "        This is a model to calculate the snow water equivalent out of given snow heights. The model is ported from R\n",
    "        code which was written from Winkler et al. 2020 \"Snow Water Equivalents exclusively from Snow Heights and their\n",
    "        temporal Changes: The âˆ†SNOW.MODEL\".\n",
    "\n",
    "        The corresponding R model can be found here: https://r-forge.r-project.org/projects/nixmass/\n",
    "\n",
    "        The code structure was adapted a bit but the calculations, available input and output variables stayed the same.\n",
    "\n",
    "        Ported by Manuel Theurl who is taking no warranties for the correctness of the R to python port.\n",
    "\n",
    "        Please refer to the official paper (lays in root of repo) for further description of the parameters.\n",
    "\n",
    "        :param rho_max: Maximum Density\n",
    "        :param rho_null: Fresh Snow Density\n",
    "        :param c_ov: Overburden Parameter\n",
    "        :param k_ov: Overburden Parameter\n",
    "        :param k: Viscosity Parameter\n",
    "        :param tau: Discrepancy Parameter\n",
    "        :param eta_null: Viscosity Parameter\n",
    "        \"\"\"\n",
    "        self.rho_max = rho_max\n",
    "        self.rho_null = rho_null\n",
    "        self.c_ov = c_ov\n",
    "        self.k_ov = k_ov\n",
    "        self.k = k\n",
    "        self.tau = tau\n",
    "        self.eta_null = eta_null\n",
    "\n",
    "        self.prec = 10 ** -10  # precision for arithmetic comparisons [-]\n",
    "        self._snowpack_dd = 0  # will get reset on every new convert\n",
    "        self._current_day_info_string = ''  # information gets added freshly for every day\n",
    "\n",
    "        print(\"This model is ported from R code which was written by Winkler et al. 2020 'Snow Water Equivalents \"\n",
    "              \"exclusively from Snow Heights and their temporal Changes: The âˆ†SNOW.MODEL'\")\n",
    "\n",
    "    def convert_list(self, Hobs, timestep, verbose=False):\n",
    "        \"\"\"\n",
    "        Converts a continuous time series of snow heights into snow water equivalents.\n",
    "\n",
    "        :param Hobs:  List of snow heights evenly spaced by timestep\n",
    "        :param timestep: in hours\n",
    "        :param verbose: bool for printing out information about the ongoing process\n",
    "        :return: List of swes on success else None\n",
    "        \"\"\"\n",
    "\n",
    "        if any(np.isnan(Hobs)):\n",
    "            print(\"swe.deltasnow: snow depth data must not be NA\")\n",
    "            return\n",
    "        if not all([x >= 0 for x in Hobs]):\n",
    "            print(\"swe.deltasnow: snow depth data must not be negative\")\n",
    "            return\n",
    "        if not all([np.isreal(x) for x in Hobs]):\n",
    "            print(\"swe.deltasnow: snow depth data must be numeric\")\n",
    "            return\n",
    "        if Hobs[0]:\n",
    "            print(\"swe.deltasnow: snow depth observations must start with 0\")\n",
    "            return\n",
    "\n",
    "        ts = timestep * 3600  # timestep between observations [s]\n",
    "        self._snowpack_dd = 0  # reset\n",
    "\n",
    "        H = []  # modeled total height of snow at any day [m]\n",
    "        SWE = []  # modeled total SWE at any day [kg/m2]\n",
    "        ly = 1  # layer number [-]\n",
    "\n",
    "        # preallocate matrix as days X layers\n",
    "        ly_tot = np.count_nonzero(Hobs)  # maximum number of layers [-]\n",
    "        day_tot = len(Hobs)  # total days from first to last snowfall [-]\n",
    "\n",
    "        h = np.zeros((ly_tot, day_tot))  # modeled height of snow in all layers [m]\n",
    "        swe = np.zeros((ly_tot, day_tot))  # modeled swe in all layers [kg/m2]\n",
    "        age = np.zeros((ly_tot, day_tot))  # age of modeled layers [days]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Using parameters:\")\n",
    "            print(\"rho.max  =\", self.rho_max)\n",
    "            print(\"rho.null =\", self.rho_null)\n",
    "            print(\"c.ov     =\", self.c_ov)\n",
    "            print(\"k.ov     =\", self.k_ov)\n",
    "            print(\"k        =\", self.k)\n",
    "            print(\"tau      =\", self.tau)\n",
    "            print(\"eta.null =\", self.eta_null)\n",
    "\n",
    "        for t in range(day_tot):\n",
    "            self._current_day_info_string = f\"day {t+1}: \"\n",
    "\n",
    "            # SnowDepth_m = 0, no snow cover\n",
    "            if Hobs[t] == 0:\n",
    "                if t > 0:\n",
    "                    if Hobs[t - 1] != 0:\n",
    "                        self._current_day_info_string += \"runoff\"\n",
    "\n",
    "                try:  # actually brutally bad written, but whatever\n",
    "                    H[t] = 0  # DIFFERENCE: H is a number, cannot index to it, in R you can\n",
    "                    SWE[t] = 0\n",
    "                except IndexError:\n",
    "                    H.append(0)\n",
    "                    SWE.append(0)\n",
    "                h[:, t] = 0  # first column to 0\n",
    "                swe[:, t] = 0\n",
    "            # there is snow\n",
    "            elif Hobs[t] > 0:  # redundant if, cause can snow height be negative?\n",
    "                # first snow in/during season\n",
    "                if Hobs[t - 1] == 0:\n",
    "                    ly = 1\n",
    "                    self._current_day_info_string += f\"produce layer {ly} \"\n",
    "                    age[ly - 1, t] = 1\n",
    "                    h[ly - 1, t] = Hobs[t]\n",
    "                    H.append(Hobs[t])  # DIFFERENCE: H is a number, cannot index to it, in R you can\n",
    "                    swe[ly - 1, t] = self.rho_null * Hobs[t]\n",
    "                    SWE.append(swe[ly - 1, t])  # DIFFERENCE: SWE is a number, cannot index to it, in R you can\n",
    "\n",
    "                    # compact actual day\n",
    "                    snowpack_tomorrow = self.__dry_metamorphism(h[:, t], swe[:, t], age[:, t], ly_tot, ly, ts)\n",
    "\n",
    "                    rl = self.__assignH(snowpack_tomorrow, h, swe, age, H, SWE, t, day_tot)\n",
    "                    h = rl['h']\n",
    "                    swe = rl[\"swe\"]\n",
    "                    age = rl[\"age\"]\n",
    "                    H = rl['H']\n",
    "                    SWE = rl[\"SWE\"]\n",
    "\n",
    "                elif Hobs[t - 1] > 0:\n",
    "                    deltaH = Hobs[t] - H[t]\n",
    "\n",
    "                    if deltaH > self.tau:\n",
    "                        self._current_day_info_string += f\"create new layer {ly} \"\n",
    "                        sigma_null = deltaH * self.rho_null * EARTH_GRAVITATIONAL_ACCELERATION\n",
    "                        epsilon = self.c_ov * sigma_null * exp(-self.k_ov * self._snowpack_dd[\"rho\"] / (self.rho_max - self._snowpack_dd[\"rho\"]))\n",
    "                        h[:, t] = (1 - epsilon) * h[:, t]\n",
    "                        # epsilon <- 1 - c.ov * sigma.null * exp(-k.ov * snowpack.dd$rho/(rho.max - snowpack.dd$rho))\n",
    "                        # h[,t]     <- epsilon * h[,t]\n",
    "\n",
    "                        swe[:, t] = swe[:, t - 1]\n",
    "                        age[:ly, t] = age[:ly, t - 1] + 1\n",
    "\n",
    "                        H[t] = sum(h[:, t])\n",
    "                        SWE[t] = sum(swe[:, t])\n",
    "\n",
    "                        # RHO[t]    <- SWE[t]/H[t]\n",
    "\n",
    "                        # only for new layer\n",
    "                        ly = ly + 1\n",
    "                        h[ly - 1, t] = Hobs[t] - H[t]\n",
    "                        swe[ly - 1, t] = self.rho_null * h[ly - 1, t]\n",
    "                        age[ly - 1, t] = 1\n",
    "\n",
    "                        # recompute\n",
    "                        H[t] = sum(h[:, t])\n",
    "\n",
    "                        SWE[t] = sum(swe[:, t])\n",
    "\n",
    "                        # compact actual day\n",
    "                        snowpack_tomorrow = self.__dry_metamorphism(h[:, t], swe[:, t], age[:, t], ly_tot, ly, ts)\n",
    "                        # set values for next day\n",
    "                        rl = self.__assignH(snowpack_tomorrow, h, swe, age, H, SWE, t, day_tot)\n",
    "                        h = rl[\"h\"]\n",
    "                        swe = rl[\"swe\"]\n",
    "                        age = rl[\"age\"]\n",
    "                        H = rl[\"H\"]\n",
    "                        SWE = rl[\"SWE\"]\n",
    "\n",
    "                    # no mass gain or loss, but scaling\n",
    "                    elif -self.tau <= deltaH <= self.tau:\n",
    "                        self._current_day_info_string += \"scaling: \"\n",
    "                        rl = self.__scaleH(t, ly, ly_tot, day_tot, deltaH, Hobs, h, swe, age, H, SWE, ts)\n",
    "                        h = rl[\"h\"]\n",
    "                        swe = rl[\"swe\"]\n",
    "                        age = rl[\"age\"]\n",
    "                        H = rl[\"H\"]\n",
    "                        SWE = rl[\"SWE\"]\n",
    "\n",
    "                    elif deltaH < -self.tau:\n",
    "                        self._current_day_info_string += \"drenching: \"\n",
    "                        rl = self.__drenchH(t, ly, ly_tot, day_tot, Hobs, h, swe, age, H, SWE, ts)\n",
    "                        h = rl[\"h\"]\n",
    "                        swe = rl[\"swe\"]\n",
    "                        age = rl[\"age\"]\n",
    "                        H = rl[\"H\"]\n",
    "                        SWE = rl[\"SWE\"]\n",
    "\n",
    "                    else:\n",
    "                        self._current_day_info_string += \"??\"\n",
    "            if verbose:\n",
    "                print(self._current_day_info_string)\n",
    "        return SWE\n",
    "\n",
    "    def convert_csv(self, path_to_input_csv, path_to_output_csv=None, date_time_pattern=\"%Y-%m-%d\", verbose=False):\n",
    "        \"\"\"\n",
    "        Converts a continuous time series of snow heights given in a csv file into snow water equivalents.\n",
    "\n",
    "        :param path_to_input_csv: path of a .csv file containing the data. Required columns are \"date\" with data format\n",
    "            e.g. 2020-09-09 and \"hs\" (snow height in meters) with data format e.g. 0.56\n",
    "        :param path_to_output_csv: Path where to save result csv, make sure all folders exist. If None it will not be\n",
    "            saved in file.\n",
    "        :param date_time_pattern: Pattern to convert date_time string to datetime object. Refer to python datetime\n",
    "            strptime codes to find appropriate one\n",
    "        :param verbose: bool for printing out information about the ongoing process\n",
    "        :return: pandas time series with snow water equivalents\n",
    "        \"\"\"\n",
    "\n",
    "        data = pd.read_csv(path_to_input_csv)\n",
    "        try:\n",
    "            dates = data[\"Timestamp\"].tolist()\n",
    "            time_resolution_in_seconds = self.__get_time_resolution_of_dates_in_seconds(dates, date_time_pattern)\n",
    "            Hobs = data[\"SnowDepth_m\"].tolist()\n",
    "\n",
    "            if time_resolution_in_seconds:\n",
    "                swes = self.convert_list(Hobs, time_resolution_in_seconds/3600, verbose=verbose)\n",
    "                if swes is None:\n",
    "                    print(\"SWE Conversion failed!\")\n",
    "                    return\n",
    "                result_pandas_df = pd.DataFrame(list(zip(dates, swes)), columns=[\"Timestamp\", \"swe\"])\n",
    "                if path_to_output_csv is not None:\n",
    "                    try:\n",
    "                        result_pandas_df.to_csv(path_to_output_csv, float_format='%.3f')\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"Cannot save csv result to {path_to_output_csv} .. make sure all folders exist!\")\n",
    "                return result_pandas_df\n",
    "            else:\n",
    "                print(\"Problems with time series dates\")\n",
    "                return None\n",
    "        except KeyError:\n",
    "            print(\"Wrong .csv file format! Required columns are 'date' with data format e.g. 2020-09-09 and 'hs' (snow \"\n",
    "                  \"height in meters) with data format e.g. 0.56\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def __assignH(sp_dd, h, swe, age, H, SWE, t, day_tot):\n",
    "        if t < day_tot:\n",
    "            h[:, t + 1] = sp_dd['h']\n",
    "            swe[:, t + 1] = sp_dd[\"swe\"]\n",
    "            age[:, t + 1] = sp_dd[\"age\"]\n",
    "            H.append(sum(h[:, t + 1]))\n",
    "            SWE.append(sum(swe[:, t + 1]))\n",
    "\n",
    "        return {'h': h, \"swe\": swe, \"age\": age, 'H': H, \"SWE\": SWE}\n",
    "\n",
    "    def __compactH(self, x, ts):\n",
    "        # .d  -> today\n",
    "        # .dd -> tomorrow\n",
    "        age_d = 0 if x[0] == 0 else x[3]\n",
    "        h_dd = x[0] / (1 + (x[2] * EARTH_GRAVITATIONAL_ACCELERATION * ts) / self.eta_null * exp(-self.k * x[1] / x[0]))\n",
    "        h_dd = x[1] / self.rho_max if x[1] / h_dd > self.rho_max else h_dd\n",
    "        h_dd = 0 if x[0] == 0 else h_dd\n",
    "        swe_dd = x[1]\n",
    "        age_dd = 0 if x[0] == 0 else age_d + 1\n",
    "        rho_dd = 0 if x[0] == 0 else swe_dd / h_dd\n",
    "        rho_dd = self.rho_max if self.rho_max - rho_dd < self.prec else rho_dd\n",
    "        # return [h_dd, swe_dd, age_dd, rho_dd]\n",
    "        # return x\n",
    "        df = pd.DataFrame(columns=['h', \"swe\", \"age\", \"rho\"])\n",
    "        df.loc[0] = [h_dd, swe_dd, age_dd, rho_dd]\n",
    "\n",
    "        return pd.Series([h_dd, swe_dd, age_dd, rho_dd], index=['h', \"swe\", \"age\", \"rho\"])\n",
    "\n",
    "    def __scaleH(self, t, ly, ly_tot, day_tot, deltaH, Hobs, h, swe, age, H, SWE, ts):\n",
    "        # re-compact snowpack from yesterdays values with adapted eta\n",
    "        # .d  -> yesterday\n",
    "        # .dd -> today\n",
    "        Hobs_d = Hobs[t - 1]\n",
    "\n",
    "        Hobs_dd = Hobs[t]\n",
    "        h_d = h[:, t - 1]\n",
    "        swe_d = swe[:, t - 1]\n",
    "        age_d = age[:, t]  # ; deltaH.d = deltaH\n",
    "\n",
    "        # todays overburden\n",
    "        swe_hat_d = []\n",
    "        for i in range(ly_tot):\n",
    "            swe_hat_d.append(sum(swe_d[i:ly_tot]))\n",
    "\n",
    "        # analytical solution for layerwise adapted viskosity eta\n",
    "        # assumption: recompaction ~ linear height change of yesterdays layers (see paper)\n",
    "        eta_cor = []\n",
    "        for i in range(ly_tot):\n",
    "            rho_d = swe_d[i] / h_d[i]\n",
    "            x = ts * EARTH_GRAVITATIONAL_ACCELERATION * swe_hat_d[i] * exp(-self.k * rho_d)  # yesterday\n",
    "            P = h_d[i] / Hobs_d  # yesterday\n",
    "            eta_i = Hobs_dd * x * P / (h_d[i] - Hobs_dd * P)\n",
    "            eta_cor.append(0 if np.isnan(eta_i) else eta_i)\n",
    "\n",
    "        # compute H of today with corrected eta\n",
    "        # so that modeled H = Hobs\n",
    "        h_dd_cor = np.array(h_d) / (1 + (np.array(swe_hat_d) * EARTH_GRAVITATIONAL_ACCELERATION * ts) / np.array(eta_cor) * exp(\n",
    "            -self.k * np.array(swe_d) / np.array(h_d)))\n",
    "        h_dd_cor[np.isnan(h_dd_cor)] = 0  # replace nan with 0\n",
    "        H_dd_cor = sum(h_dd_cor)\n",
    "\n",
    "        # and check, if Hd.cor is the same as Hobs.d\n",
    "        if abs(H_dd_cor - Hobs_dd) > self.prec:\n",
    "            self._current_day_info_string += f\"WARNING: error in exponential re-compaction: H.dd.cor-Hobs.dd='{H_dd_cor - Hobs_dd}'\"\n",
    "\n",
    "        # which layers exceed rho.max?\n",
    "        idx_max = []\n",
    "        for i, (swe_e_val, h_dd_cor_val) in enumerate(zip(swe_d, h_dd_cor)):\n",
    "            if swe_e_val / h_dd_cor_val - self.rho_max > self.prec:\n",
    "                idx_max.append(i)\n",
    "\n",
    "        # idx_max = np.where(, swe_d, h_dd_cor)[0]  # [0] cause tuple with list is returned\n",
    "        if len(idx_max) > 0:\n",
    "            if len(idx_max) < ly:\n",
    "                # collect excess swe in those layers\n",
    "                swe_excess = swe_d[idx_max] - h_dd_cor[idx_max] * self.rho_max\n",
    "\n",
    "                # set affected layer(s) to rho.max\n",
    "                swe_d[idx_max] = swe_d[idx_max] - swe_excess\n",
    "\n",
    "                # distribute excess swe to other layers top-down\n",
    "                lys = list(range(ly))\n",
    "                for index in sorted(idx_max, reverse=True):\n",
    "                    del lys[index]\n",
    "                i = lys[len(lys) - 1]\n",
    "                swe_excess_all = sum(swe_excess)\n",
    "\n",
    "                while swe_excess_all > 0:\n",
    "                    swe_res = h_dd_cor[i] * self.rho_max - swe_d[i]  # layer tolerates this swe amount to reach rho.max\n",
    "                    if swe_res > swe_excess_all:\n",
    "                        swe_res = swe_excess_all\n",
    "\n",
    "                    swe_d[i] = swe_d[i] + swe_res\n",
    "                    swe_excess_all = swe_excess_all - swe_res\n",
    "                    i = i - 1\n",
    "                    if i < 0 < swe_excess_all:\n",
    "                        self._current_day_info_string += \" runoff\"\n",
    "                        break\n",
    "            else:\n",
    "                # if all layers have density > rho.max\n",
    "                # remove swe.excess from all layers (-> runoff)\n",
    "                # (this sets density to rho.max)\n",
    "                swe_excess = swe_d[idx_max] - h_dd_cor[idx_max] * self.rho_max\n",
    "                swe_d[idx_max] = swe_d[idx_max] - swe_excess\n",
    "                self._current_day_info_string += \" runoff\"\n",
    "\n",
    "        h[:, t] = h_dd_cor\n",
    "        swe[:, t] = swe_d\n",
    "        age[:, t] = age_d\n",
    "        H[t] = sum(h[:, t])\n",
    "        SWE[t] = sum(swe[:, t])\n",
    "\n",
    "        # compact actual day\n",
    "        # if all layers already have maximum density rho_max\n",
    "        # the snowpack will not be changed by the following step\n",
    "        # nonlocal or not?????\n",
    "        snowpack_tomorrow = self.__dry_metamorphism(h[:, t], swe[:, t], age[:, t], ly_tot, ly, ts)\n",
    "\n",
    "        # set values for next day\n",
    "        rl = self.__assignH(snowpack_tomorrow, h, swe, age, H, SWE, t, day_tot)\n",
    "        h = rl[\"h\"]\n",
    "        swe = rl[\"swe\"]\n",
    "        age = rl[\"age\"]\n",
    "        H = rl[\"H\"]\n",
    "        SWE = rl[\"SWE\"]\n",
    "\n",
    "        return {'h': h, \"swe\": swe, \"age\": age, 'H': H, \"SWE\": SWE}\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_time_resolution_of_dates_in_seconds(dates, date_time_pattern=\"%Y-%m-%d\"):\n",
    "        last_delta = None\n",
    "        last_date = None\n",
    "        for date_string in dates:\n",
    "            try:\n",
    "                current_date = dt.datetime.strptime(date_string, date_time_pattern)\n",
    "            except ValueError:\n",
    "                print(f\"Wrong date_time pattern {date_time_pattern}! Refer to python datetime strptime codes to find \"\n",
    "                      f\"appropriate!\")\n",
    "                return False\n",
    "            if last_date is not None:\n",
    "                current_delta = current_date - last_date\n",
    "\n",
    "                if last_delta is not None and last_delta != current_delta:\n",
    "                    print(\"Time series is not evenly spaced\")\n",
    "                    return False\n",
    "\n",
    "                last_delta = current_delta\n",
    "            last_date = current_date\n",
    "        return last_delta.total_seconds()\n",
    "\n",
    "    def __dry_metamorphism(self, h_d, swe_d, age_d, ly_tot, ly, ts):\n",
    "        # h.d=h[,t];swe.d=swe[,t];age.d=age[,t]\n",
    "        # snowpack.dd <- NULL\n",
    "        # .d  -> today\n",
    "        # .dd -> tomorrow\n",
    "\n",
    "        # compute overburden for each layer\n",
    "        # the overburden for the first layer is the layer itself\n",
    "\n",
    "        swe_hat_d = []\n",
    "        for i in range(ly_tot):\n",
    "            swe_hat_d.append(sum(swe_d[i:ly_tot]))\n",
    "\n",
    "        # dictionary of lists\n",
    "        snowpack_d = pd.DataFrame({'h': h_d, \"swe\": swe_d, \"swe_hat\": swe_hat_d, \"age\": age_d})\n",
    "        H_d = sum(snowpack_d['h'])\n",
    "\n",
    "        a = snowpack_d.head(ly).apply(self.__compactH, axis=1, args=(ts, ))\n",
    "        b = pd.DataFrame(np.zeros((ly_tot - ly, 4)))\n",
    "        b.columns = ['h', \"swe\", \"age\", \"rho\"]\n",
    "\n",
    "        self._snowpack_dd = pd.concat([a, b])\n",
    "        # rownames(snowpack.dd.row) << - self.paste0(\"dd.layer\", 1: nrow(snowpack.dd))\n",
    "        return self._snowpack_dd\n",
    "\n",
    "    def __drenchH(self, t, ly, ly_tot, day_tot, Hobs, h, swe, age, H, SWE, ts):\n",
    "        Hobs_d = Hobs[t]\n",
    "        h_d = h[:, t]\n",
    "        swe_d = swe[:, t]\n",
    "        age_d = age[:, t]\n",
    "\n",
    "        self._current_day_info_string += \"melt \"\n",
    "\n",
    "        runoff = 0\n",
    "        # distribute mass top-down\n",
    "        for i in reversed(range(ly)):\n",
    "            if sum([element for j, element in enumerate(h_d) if j != i]) + swe_d[i] / self.rho_max - Hobs_d >= self.prec:\n",
    "                # layers is densified to rho_max\n",
    "                h_d[i] = swe_d[i] / self.rho_max\n",
    "            else:\n",
    "                # layer is densified as far as possible\n",
    "                # but doesnt reach rho_max\n",
    "                h_d[i] = swe_d[i] / self.rho_max + abs(\n",
    "                    sum([element for j, element in enumerate(h_d) if j != i]) + swe_d[i] / self.rho_max - Hobs_d)\n",
    "                break\n",
    "\n",
    "        true_false_list = [self.rho_max - swe_d_val / h_d_val <= self.prec for swe_d_val, h_d_val in\n",
    "                           zip(swe_d[:ly], h_d[:ly])]\n",
    "\n",
    "        if all(true_false_list):\n",
    "            self._current_day_info_string += \"no further compaction \"\n",
    "            # produce runoff if sum(h_d) - Hobs_d is still > 0\n",
    "            self._current_day_info_string += \"runoff \"\n",
    "            # decrease swe from all layers?\n",
    "            # or beginning with lowest?\n",
    "            # swe_d[1:ly] <- swe_d[1:ly] - (sum(h_d) - Hobs_d) * rho_max\n",
    "            scale = Hobs_d / sum(h_d)\n",
    "            runoff = (sum(h_d) - Hobs_d) * self.rho_max  # excess is converted to runoff [kg/m2]\n",
    "            h_d = h_d * scale  # all layers are compressed (and have rho_max) [m]\n",
    "            swe_d = swe_d * scale\n",
    "            # self._current_day_info_string += str(runoff)\n",
    "\n",
    "        else:\n",
    "            self._current_day_info_string += \"compaction \"\n",
    "\n",
    "        h[:, t] = h_d\n",
    "        swe[:, t] = swe_d\n",
    "        age[:, t] = age_d\n",
    "        H[t] = sum(h[:, t])\n",
    "        SWE[t] = sum(swe[:, t])\n",
    "        #\n",
    "        # no further compaction possible\n",
    "        # snowpack_tomorrow <- cbind(h = h_d, swe = swe_d, age = age_d, rho = swe_d/h_d)\n",
    "        # colnames(snowpack_tomorrow) <- c(\"h\",\"swe\",\"age\",\"rho\")\n",
    "\n",
    "        snowpack_tomorrow = self.__dry_metamorphism(h[:, t], swe[:, t], age[:, t], ly_tot, ly, ts)\n",
    "\n",
    "        # set values for next day\n",
    "        rl = self.__assignH(snowpack_tomorrow, h, swe, age, H, SWE, t, day_tot)\n",
    "        h = rl[\"h\"]\n",
    "        swe = rl[\"swe\"]\n",
    "        age = rl[\"age\"]\n",
    "        H = rl[\"H\"]\n",
    "        SWE = rl[\"SWE\"]\n",
    "\n",
    "        return {'h': h, \"swe\": swe, \"age\": age, 'H': H, \"SWE\": SWE}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_hsdata = \"Data/Cleaned/seasonal_SDepth_filled.csv\"\n",
    "    snow_to_swe = SnowToSwe()\n",
    "    swe_pandas_df = snow_to_swe.convert_csv(path_to_hsdata, path_to_output_csv=\"Data/Cleaned/seasonal_out.csv\", verbose=True)\n",
    "    hs_data_as_list = pd.read_csv(path_to_hsdata)[\"SnowDepth_m\"].tolist()\n",
    "    swe_list = snow_to_swe.convert_list(hs_data_as_list, 24, verbose=True)\n",
    "\n",
    "    # results match with given R model example of Winkler et al. 2020 (git repo)\n",
    "    print(\"Mean\", np.mean(swe_list))\n",
    "    print(\"Max\", max(swe_list))\n",
    "    print(\"Sum\", sum(swe_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd620ee-7e1d-4214-99a6-a1ce3f8f463b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
